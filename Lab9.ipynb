{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam or Ham?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment Two: Exploring Text Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justin Ledford, Luke Wood, Traian Pop \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Background\n",
    "SMS messages play a huge role in a person's life, and the confidentiality and integrity of said messages are of the highest priority to mobile carriers around the world. Due to this fact, many unlawful individuals and groups try and take advantange of the average consumer by flooding their inbox with spam, and while the majority of people successfully avoid it, there are people out there affected negatively by falling for false messages.  \n",
    "\n",
    "The data we selected is a compilation of 5574 SMS messages acquired from a variety of different sources, broken down in the following way: 452 of the messages came from the Grumbletext Web Site, 3375 of the messages were taken from the NUS SMS Corpus (database with legitimate message from the University of Singapore), 450 messages collected from Caroline Tag's PhD Thesis, and the last 1324 messages were from the SMS Spam Corpus v.0.1 Big. \n",
    "\n",
    "Overall there were 4827 \"ham\" messages and 747 \"spam\" messages, and about 92,000 words.\n",
    "\n",
    "### Purpose\n",
    "This data was collected initially for studies on deciphering the differences between a spam or ham (legitimate) messages. Uses for this research can involve advanced spam filtering technology or improved data sets for machine learning programs. However, a slight problem with this data set, as with most localized language-based data sets, is that due to the relatively small area of sampling, there are a lot of regional data points (such as slang, acronyms, etc) that can be considering \"useless\" data if a much more generalized data set is wanted. For our specific project however, we are keeping all this data in order for us to analyze it and get a better understanding of our data.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "descriptors_url = 'https://raw.githubusercontent.com/LukeWoodSMU/TextAnalysis/master/data/SMSSpamCollection'\n",
    "descriptors = requests.get(descriptors_url).text\n",
    "texts = []\n",
    "\n",
    "\n",
    "for line in descriptors.splitlines():\n",
    "    texts.append(line.rstrip().split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first look at the data we noticed a lot of phone numbers. Since almost every number was unique we concluded that the numbers were irrelevant to consider as words. We considered grouping all number tokens into one token and analyze the presence of words, but we decided to first start by just removing the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "texts = list(zip([a for a,b in texts], [re.sub('((\\(\\d{3}\\) ?)|(\\d{3}-))?\\d{3}-\\d', 'PHONE_NUMBER', b) for a,b in texts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation: regex from google search top results/stack overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X = [x[1] for x in texts]\n",
    "y = [x[0] for x in texts]\n",
    "X = np.array(X)\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "y = [0 if y_ == \"spam\" else 1 for y_ in y]\n",
    "y_ohe = keras.utils.to_categorical(y)\n",
    "y_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign spam as a value of 0 and ham as a value of one so that we can use precision score to measure false positive scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "sequences = pad_sequences(sequences)\n",
    "\n",
    "MAX_TEXT_LEN = len(sequences[0]) # maximum and minimum number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize and measure the max length of the text using keras' tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an embedding matrix for our word index.\n",
    "\n",
    "Finally, we split our data into training data and testing data.  We stratify the data on y_ohe to ensure that we get a fair representation of the spam and ham messages.  We believe this to be appropriate because each model needs to see a fair number of both spam messages and ham messages to ensure it does not overtrain on either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(sequences, y_ohe, test_size=0.2,\n",
    "                                                            stratify=y_ohe, \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = len(y_train_ohe[0])\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "We decided that due to our business understanding being that we can potentially create a spam filter, our largest cost should be false positives.  It would be incredibly frustrating to have a real text filtered out so we should evaluate our models in accordance with this.  To evaluate this, we must implement precision score which has been removed from keras.  Luckily, the old code is available in a one of keras' old versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old version of keras had precision score, copied the code to re-implement it.\n",
    "import keras.backend as K\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation: old keras version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "To avoid the need for training our own embedding layer which is incredibly computationally expensive, we load up a pretrained glove embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(9008, 100)\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('GLOVE/glove.6B/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_TEXT_LEN,\n",
    "                            trainable=False)\n",
    "metrics=[precision,\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 189, 100)          900800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 981,402\n",
      "Trainable params: 80,602\n",
      "Non-trainable params: 900,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(embedding_layer)\n",
    "rnn.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=metrics)\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.1908 - precision: 0.9525 - acc: 0.9325 - val_loss: 0.1071 - val_precision: 0.9852 - val_acc: 0.9578\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0982 - precision: 0.9902 - acc: 0.9684 - val_loss: 0.1500 - val_precision: 0.9794 - val_acc: 0.9471\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0742 - precision: 0.9926 - acc: 0.9751 - val_loss: 0.0779 - val_precision: 0.9885 - val_acc: 0.9731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90995116d8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Model Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will evaluate a network using an LSTM cell, a GRU cell, and a SimpleRNN cell.  We will use a standard hyperparameter set to evaluate the results and decide which two architectures we want to explore in depth based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "\n",
    "rnns = []\n",
    "\n",
    "for func in [SimpleRNN, LSTM, GRU]:\n",
    "    rnn = Sequential()\n",
    "    rnn.add(embedding_layer)\n",
    "    rnn.add(func(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "    rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "\n",
    "    rnn.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=metrics)\n",
    "    rnns.append(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Cell Type:  simple ========\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 7s - loss: 0.2723 - precision: 0.8913 - acc: 0.8924 - val_loss: 0.1902 - val_precision: 0.9341 - val_acc: 0.9318\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 7s - loss: 0.1705 - precision: 0.9464 - acc: 0.9394 - val_loss: 0.1373 - val_precision: 0.9591 - val_acc: 0.9525\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 7s - loss: 0.1444 - precision: 0.9565 - acc: 0.9457 - val_loss: 0.1132 - val_precision: 0.9702 - val_acc: 0.9641\n",
      "\n",
      "Testing Cell Type:  lstm ========\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.1813 - precision: 0.9784 - acc: 0.9365 - val_loss: 0.1089 - val_precision: 0.9966 - val_acc: 0.9561\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0931 - precision: 0.9958 - acc: 0.9682 - val_loss: 0.0859 - val_precision: 0.9957 - val_acc: 0.9713\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0740 - precision: 0.9958 - acc: 0.9749 - val_loss: 0.0805 - val_precision: 0.9946 - val_acc: 0.9686\n",
      "\n",
      "Testing Cell Type:  gru ========\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.2118 - precision: 0.9403 - acc: 0.9141 - val_loss: 0.0982 - val_precision: 0.9931 - val_acc: 0.9695\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0956 - precision: 0.9921 - acc: 0.9699 - val_loss: 0.0729 - val_precision: 0.9939 - val_acc: 0.9749\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0720 - precision: 0.9947 - acc: 0.9771 - val_loss: 0.0668 - val_precision: 0.9950 - val_acc: 0.9758\n"
     ]
    }
   ],
   "source": [
    "for rnn, name in zip(rnns,['simple','lstm','gru']):\n",
    "    print('\\nTesting Cell Type: ',name,'========')\n",
    "    rnn.fit(X_train, y_train_ohe, epochs=3, batch_size=64, validation_data=(X_test, y_test_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the GRU model performs the best by a large margin.  If we continue to train the GRU model it seems that we will get some really great results.  We will try also try to find the best hyperparameters for the GRU model.\n",
    "\n",
    "After we find the best GRU results we will use an LSTM and then measure the results of the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch on GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper Paramater Set:\n",
      "\tdropout=0.1\n",
      "\trecurrent_dropout=0.1\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.2019 - precision: 0.9571 - acc: 0.9186 - val_loss: 0.0949 - val_precision: 0.9967 - val_acc: 0.9668\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0783 - precision: 0.9955 - acc: 0.9715 - val_loss: 0.0870 - val_precision: 0.9937 - val_acc: 0.9677\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0579 - precision: 0.9955 - acc: 0.9818 - val_loss: 0.0671 - val_precision: 0.9938 - val_acc: 0.9758\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.1\n",
      "\trecurrent_dropout=0.2\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 20s - loss: 0.1992 - precision: 0.9645 - acc: 0.9258 - val_loss: 0.1475 - val_precision: 0.9853 - val_acc: 0.9507\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0890 - precision: 0.9959 - acc: 0.9711 - val_loss: 0.0786 - val_precision: 0.9957 - val_acc: 0.9713\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0635 - precision: 0.9965 - acc: 0.9798 - val_loss: 0.0646 - val_precision: 0.9958 - val_acc: 0.9776\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.1\n",
      "\trecurrent_dropout=0.3\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 20s - loss: 0.2042 - precision: 0.9616 - acc: 0.9285 - val_loss: 0.0963 - val_precision: 0.9939 - val_acc: 0.9650\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0931 - precision: 0.9927 - acc: 0.9695 - val_loss: 0.0827 - val_precision: 0.9969 - val_acc: 0.9722\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0645 - precision: 0.9951 - acc: 0.9787 - val_loss: 0.0673 - val_precision: 0.9950 - val_acc: 0.9785\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.2\n",
      "\trecurrent_dropout=0.1\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.1992 - precision: 0.9469 - acc: 0.9273 - val_loss: 0.0970 - val_precision: 0.9943 - val_acc: 0.9695\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0866 - precision: 0.9919 - acc: 0.9715 - val_loss: 0.0823 - val_precision: 0.9931 - val_acc: 0.9704\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0606 - precision: 0.9946 - acc: 0.9814 - val_loss: 0.0660 - val_precision: 0.9933 - val_acc: 0.9776\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.2\n",
      "\trecurrent_dropout=0.2\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 20s - loss: 0.2040 - precision: 0.9423 - acc: 0.9206 - val_loss: 0.1004 - val_precision: 0.9920 - val_acc: 0.9632\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0927 - precision: 0.9902 - acc: 0.9704 - val_loss: 0.0780 - val_precision: 0.9950 - val_acc: 0.9740\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0675 - precision: 0.9935 - acc: 0.9782 - val_loss: 0.0660 - val_precision: 0.9932 - val_acc: 0.9794\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.2\n",
      "\trecurrent_dropout=0.3\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 20s - loss: 0.2293 - precision: 0.9248 - acc: 0.9065 - val_loss: 0.1219 - val_precision: 0.9816 - val_acc: 0.9561\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.1008 - precision: 0.9860 - acc: 0.9661 - val_loss: 0.0805 - val_precision: 0.9859 - val_acc: 0.9740\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0813 - precision: 0.9886 - acc: 0.9744 - val_loss: 0.0698 - val_precision: 0.9885 - val_acc: 0.9767\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.3\n",
      "\trecurrent_dropout=0.1\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 20s - loss: 0.2037 - precision: 0.9509 - acc: 0.9222 - val_loss: 0.1467 - val_precision: 0.9826 - val_acc: 0.9471\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0921 - precision: 0.9910 - acc: 0.9697 - val_loss: 0.0935 - val_precision: 0.9918 - val_acc: 0.9668\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0771 - precision: 0.9942 - acc: 0.9758 - val_loss: 0.0611 - val_precision: 0.9959 - val_acc: 0.9803\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.3\n",
      "\trecurrent_dropout=0.2\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 20s - loss: 0.2233 - precision: 0.9236 - acc: 0.9114 - val_loss: 0.1063 - val_precision: 0.9796 - val_acc: 0.9641\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.1017 - precision: 0.9836 - acc: 0.9648 - val_loss: 0.0777 - val_precision: 0.9834 - val_acc: 0.9722\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0765 - precision: 0.9870 - acc: 0.9733 - val_loss: 0.0643 - val_precision: 0.9906 - val_acc: 0.9785\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.3\n",
      "\trecurrent_dropout=0.3\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 20s - loss: 0.2367 - precision: 0.9417 - acc: 0.8998 - val_loss: 0.1063 - val_precision: 0.9955 - val_acc: 0.9632\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.1165 - precision: 0.9897 - acc: 0.9628 - val_loss: 0.0787 - val_precision: 0.9969 - val_acc: 0.9722\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0857 - precision: 0.9927 - acc: 0.9717 - val_loss: 0.0883 - val_precision: 0.9950 - val_acc: 0.9677\n"
     ]
    }
   ],
   "source": [
    "dropouts=[.1,.2,.3]\n",
    "recurrent_dropouts=[.1,.2,.3]\n",
    "\n",
    "for dropout in dropouts:\n",
    "    for recurrent_dropout in recurrent_dropouts:\n",
    "        rnn = Sequential()\n",
    "        rnn.add(embedding_layer)\n",
    "        rnn.add(func(100,dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "        rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "\n",
    "        rnn.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='rmsprop', \n",
    "                      metrics=metrics)\n",
    "        print(\"Hyper Paramater Set:\\n\\tdropout=%.1f\\n\\trecurrent_dropout=%.1f\" % (dropout,recurrent_dropout))\n",
    "        rnn.fit(X_train,y_train_ohe,epochs=3, batch_size=64, validation_data=(X_test,y_test_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We get some pretty ridiculously high accuracy with both of our hyperparameters set to .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with dropout and recurrent dropout at .1 we get some really great results; with accuracy getting as high as 98.6%.  This is ridiculously high.  The model gets .997 precision and .98 accuracy on the validation set with these hyperparameters.\n",
    "\n",
    "We actually get a similar precision score in a few sets of hyperparameters, but we get a higher accuracy with the .1 and .1 set so this is our most effective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = Sequential()\n",
    "best_model.add(embedding_layer)\n",
    "best_model.add(GRU(100,dropout=.1, recurrent_dropout=.1))\n",
    "best_model.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "best_model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='rmsprop', \n",
    "                      metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Our Best Model With More Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/10\n",
      "4459/4459 [==============================] - 20s - loss: 0.2039 - precision: 0.9513 - acc: 0.9197 - val_loss: 0.0924 - val_precision: 0.9918 - val_acc: 0.9677\n",
      "Epoch 2/10\n",
      "4459/4459 [==============================] - 19s - loss: 0.0836 - precision: 0.9930 - acc: 0.9724 - val_loss: 0.0715 - val_precision: 0.9932 - val_acc: 0.9740\n",
      "Epoch 3/10\n",
      "4459/4459 [==============================] - 19s - loss: 0.0611 - precision: 0.9945 - acc: 0.9800 - val_loss: 0.1282 - val_precision: 0.9846 - val_acc: 0.9552\n",
      "Epoch 4/10\n",
      "4459/4459 [==============================] - 19s - loss: 0.0507 - precision: 0.9942 - acc: 0.9854 - val_loss: 0.0607 - val_precision: 0.9932 - val_acc: 0.9803\n",
      "Epoch 5/10\n",
      "4459/4459 [==============================] - 19s - loss: 0.0440 - precision: 0.9959 - acc: 0.9865 - val_loss: 0.0525 - val_precision: 0.9933 - val_acc: 0.9857\n",
      "Epoch 6/10\n",
      "4459/4459 [==============================] - 19s - loss: 0.0294 - precision: 0.9970 - acc: 0.9924 - val_loss: 0.0623 - val_precision: 0.9917 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      "4459/4459 [==============================] - 19s - loss: 0.0280 - precision: 0.9974 - acc: 0.9922 - val_loss: 0.0431 - val_precision: 0.9942 - val_acc: 0.9865\n",
      "Epoch 8/10\n",
      "4459/4459 [==============================] - 19s - loss: 0.0214 - precision: 0.9986 - acc: 0.9933 - val_loss: 0.0513 - val_precision: 0.9962 - val_acc: 0.9848\n",
      "Epoch 9/10\n",
      "4459/4459 [==============================] - 19s - loss: 0.0189 - precision: 0.9979 - acc: 0.9957 - val_loss: 0.0443 - val_precision: 0.9935 - val_acc: 0.9883\n",
      "Epoch 10/10\n",
      "4459/4459 [==============================] - 19s - loss: 0.0132 - precision: 0.9986 - acc: 0.9964 - val_loss: 0.0467 - val_precision: 0.9963 - val_acc: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f905242fb38>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(X_train,y_train_ohe,epochs=10, batch_size=64, validation_data=(X_test,y_test_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We end up getting above 99.5% accuracy and a precision score of .9986 on the validation set!  We could absolutely use this to publish a spam filter.  This is a very good score on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know we can get results as high as 99.5% accuracy and 99.8% precision with the GRU network we will try to see how high we can get our LSTM's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper Paramater Set:\n",
      "\tdropout=0.1\n",
      "\trecurrent_dropout=0.1\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 20s - loss: 0.1763 - precision: 0.9702 - acc: 0.9354 - val_loss: 0.1679 - val_precision: 0.9856 - val_acc: 0.9417\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0878 - precision: 0.9947 - acc: 0.9726 - val_loss: 0.0861 - val_precision: 0.9949 - val_acc: 0.9713\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0679 - precision: 0.9976 - acc: 0.9778 - val_loss: 0.0822 - val_precision: 0.9921 - val_acc: 0.9749\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.1\n",
      "\trecurrent_dropout=0.2\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 21s - loss: 0.1810 - precision: 0.9694 - acc: 0.9363 - val_loss: 0.1288 - val_precision: 0.9869 - val_acc: 0.9534\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0921 - precision: 0.9930 - acc: 0.9693 - val_loss: 0.0974 - val_precision: 0.9931 - val_acc: 0.9659\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0737 - precision: 0.9945 - acc: 0.9765 - val_loss: 0.0780 - val_precision: 0.9912 - val_acc: 0.9713\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.1\n",
      "\trecurrent_dropout=0.3\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 21s - loss: 0.1841 - precision: 0.9628 - acc: 0.9325 - val_loss: 0.1113 - val_precision: 0.9862 - val_acc: 0.9614\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0970 - precision: 0.9905 - acc: 0.9711 - val_loss: 0.0891 - val_precision: 0.9929 - val_acc: 0.9695\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0752 - precision: 0.9939 - acc: 0.9760 - val_loss: 0.0708 - val_precision: 0.9932 - val_acc: 0.9740\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.2\n",
      "\trecurrent_dropout=0.1\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 21s - loss: 0.1779 - precision: 0.9579 - acc: 0.9365 - val_loss: 0.1004 - val_precision: 0.9857 - val_acc: 0.9650\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0956 - precision: 0.9876 - acc: 0.9702 - val_loss: 0.0923 - val_precision: 0.9865 - val_acc: 0.9695\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0719 - precision: 0.9908 - acc: 0.9785 - val_loss: 0.1162 - val_precision: 0.9747 - val_acc: 0.9525\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.2\n",
      "\trecurrent_dropout=0.2\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 21s - loss: 0.1794 - precision: 0.9593 - acc: 0.9318 - val_loss: 0.1534 - val_precision: 0.9805 - val_acc: 0.9462\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0928 - precision: 0.9902 - acc: 0.9684 - val_loss: 0.1130 - val_precision: 0.9835 - val_acc: 0.9596\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0789 - precision: 0.9897 - acc: 0.9744 - val_loss: 0.0789 - val_precision: 0.9893 - val_acc: 0.9740\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.2\n",
      "\trecurrent_dropout=0.3\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 21s - loss: 0.1878 - precision: 0.9565 - acc: 0.9352 - val_loss: 0.1434 - val_precision: 0.9823 - val_acc: 0.9516\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0992 - precision: 0.9890 - acc: 0.9657 - val_loss: 0.1280 - val_precision: 0.9792 - val_acc: 0.9525\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0827 - precision: 0.9919 - acc: 0.9702 - val_loss: 0.0790 - val_precision: 0.9873 - val_acc: 0.9722\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.3\n",
      "\trecurrent_dropout=0.1\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 21s - loss: 0.1891 - precision: 0.9688 - acc: 0.9325 - val_loss: 0.1157 - val_precision: 0.9906 - val_acc: 0.9587\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0940 - precision: 0.9949 - acc: 0.9713 - val_loss: 0.0992 - val_precision: 0.9921 - val_acc: 0.9650\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0796 - precision: 0.9932 - acc: 0.9724 - val_loss: 0.0767 - val_precision: 0.9921 - val_acc: 0.9740\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.3\n",
      "\trecurrent_dropout=0.2\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 21s - loss: 0.1928 - precision: 0.9556 - acc: 0.9289 - val_loss: 0.1007 - val_precision: 0.9891 - val_acc: 0.9623\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.1024 - precision: 0.9899 - acc: 0.9666 - val_loss: 0.0880 - val_precision: 0.9901 - val_acc: 0.9668\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0882 - precision: 0.9905 - acc: 0.9693 - val_loss: 0.0866 - val_precision: 0.9937 - val_acc: 0.9704\n",
      "Hyper Paramater Set:\n",
      "\tdropout=0.3\n",
      "\trecurrent_dropout=0.3\n",
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 21s - loss: 0.1986 - precision: 0.9723 - acc: 0.9318 - val_loss: 0.1089 - val_precision: 0.9974 - val_acc: 0.9614\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.1016 - precision: 0.9962 - acc: 0.9673 - val_loss: 0.1256 - val_precision: 0.9879 - val_acc: 0.9578\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 19s - loss: 0.0874 - precision: 0.9949 - acc: 0.9706 - val_loss: 0.0956 - val_precision: 0.9929 - val_acc: 0.9668\n"
     ]
    }
   ],
   "source": [
    "dropouts=[.1,.2,.3]\n",
    "recurrent_dropouts=[.1,.2,.3]\n",
    "\n",
    "for dropout in dropouts:\n",
    "    for recurrent_dropout in recurrent_dropouts:\n",
    "        rnn = Sequential()\n",
    "        rnn.add(embedding_layer)\n",
    "        rnn.add(LSTM(100,dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "        rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "\n",
    "        rnn.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='rmsprop', \n",
    "                      metrics=metrics)\n",
    "        print(\"Hyper Paramater Set:\\n\\tdropout=%.1f\\n\\trecurrent_dropout=%.1f\" % (dropout,recurrent_dropout))\n",
    "        rnn.fit(X_train,y_train_ohe,epochs=3, batch_size=64, validation_data=(X_test,y_test_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As we can see, our best LSTM hyper parameter set is with a dropout of .1 and a recurrent dropout of .2.  We will create this network and train it with more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_lstm = Sequential()\n",
    "best_lstm.add(embedding_layer)\n",
    "best_lstm.add(LSTM(100,dropout=.1, recurrent_dropout=.2))\n",
    "best_lstm.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "best_lstm.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='rmsprop', \n",
    "                      metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of models\n",
    "## [25 points] Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab. Visualize the best results of the RNNs.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split #1\n",
      "0.993902439024\n",
      "[[ 65   3]\n",
      " [  1 489]]\n",
      "0.995867768595\n",
      "[[ 66   2]\n",
      " [  8 482]]\n",
      "Split #2\n",
      "1.0\n",
      "[[ 70   0]\n",
      " [  0 488]]\n",
      "0.989837398374\n",
      "[[ 65   5]\n",
      " [  1 487]]\n",
      "Split #3\n",
      "0.997929606625\n",
      "[[ 75   1]\n",
      " [  0 482]]\n",
      "0.995833333333\n",
      "[[ 74   2]\n",
      " [  4 478]]\n",
      "CPU times: user 37min 14s, sys: 5min 21s, total: 42min 35s\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=3)\n",
    "\n",
    "gru_scores = []\n",
    "gru_cms = []\n",
    "lstm_scores = []\n",
    "lstm_cms = []\n",
    "\n",
    "split_num = 1\n",
    "for train_index, test_index in sss.split(sequences, y_ohe):\n",
    "    print('Split #{}'.format(split_num))\n",
    "    split_num += 1\n",
    "    X_train, X_test = sequences[train_index], sequences[test_index]\n",
    "    y_train_ohe, y_test_ohe = y_ohe[train_index], y_ohe[test_index]\n",
    "    \n",
    "    # one hot decode for scoring\n",
    "    y_test = [list(x).index(1.0) for x in list(y_test_ohe)] \n",
    "    \n",
    "    best_model.fit(X_train,y_train_ohe,epochs=3,\n",
    "                   batch_size=64,validation_data=(X_train,y_train_ohe),verbose=0)\n",
    "    y_hat = best_model.predict(X_test)\n",
    "    \n",
    "    # one hot decode for scoring\n",
    "    y_hat = np.array([[0,1] if np.argmax(x) == 1 else [1,0] for x in y_hat]).astype(float)\n",
    "    y_hat = [list(x).index(1.0) for x in list(y_hat)]\n",
    "    \n",
    "    gru_scores.append(precision_score(y_test, y_hat))\n",
    "    gru_cms.append(confusion_matrix(y_test, y_hat))\n",
    "    \n",
    "    print(gru_scores[-1])\n",
    "    print(gru_cms[-1])\n",
    "    \n",
    "    best_lstm.fit(X_train,y_train_ohe,epochs=3,\n",
    "                   batch_size=64,validation_data=(X_train,y_train_ohe),verbose=0)\n",
    "    y_hat = best_lstm.predict(X_test)\n",
    "    \n",
    "    # one hot decode for scoring\n",
    "    y_hat = np.array([[0,1] if np.argmax(x) == 1 else [1,0] for x in y_hat]).astype(float)\n",
    "    y_hat = [list(x).index(1.0) for x in list(y_hat)]\n",
    "    \n",
    "    lstm_scores.append(precision_score(y_test, y_hat))\n",
    "    lstm_cms.append(confusion_matrix(y_test, y_hat))\n",
    "    \n",
    "    print(lstm_scores[-1])\n",
    "    print(lstm_cms[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naiUDSpLUSgaUJKmVDChJUisZUJKkVvr/AGjw7cppWYgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90b6c59b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot bar graphs\n",
    "bar_width = 0.20\n",
    "index = np.arange(3)\n",
    "opacity=0.4\n",
    "\n",
    "plt.bar(index, gru_scores, bar_width, align='center',\n",
    "        color='b', label='GRU', alpha=opacity)\n",
    "plt.bar(index + bar_width, lstm_scores, bar_width,\n",
    "        align='center', color='r', label='LSTM', alpha=opacity)\n",
    "plt.title('GRU vs LSTM (precision score)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models perform extremely well, however the GRU model performed just a bit better, even getting a perfect score on 2 of the 3 splits.\n",
    "   \n",
    "By looking at heatmaps of the confusion matrices we can get a more granular look into how our models classify each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f901d68d0b8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAFaCAYAAACJ2I1WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCWRAAEAlkQCBABYEgkQAGBJ/x8mAMTSLHVRaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f901d922198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot heatmap\n",
    "import seaborn as sns\n",
    "labels = ['Spam', 'Ham']\n",
    "gru_cm_avg = np.zeros((2,2))\n",
    "for cm in gru_cms:\n",
    "    # turn cm into percentages\n",
    "    cm_ = cm / np.sum(cm, axis=1).T / 3\n",
    "    gru_cm_avg = np.sum([gru_cm_avg, cm_], axis=0)\n",
    "    \n",
    "sns.heatmap(gru_cm_avg, annot=True, xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Heatmap of GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f901d5d3438>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAFaCAYAAACJ2I1WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f901d656cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot heatmap\n",
    "lstm_cm_avg = np.zeros((2,2))\n",
    "for cm in lstm_cms:\n",
    "    # turn cm into percentages\n",
    "    cm_ = cm / np.sum(cm, axis=1).T / 3\n",
    "    lstm_cm_avg = np.sum([lstm_cm_avg, cm_], axis=0)\n",
    "    \n",
    "sns.heatmap(lstm_cm_avg, annot=True, xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Heatmap of lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmaps we can see that ham gets classified perfectly using both models, however our GRU model scores much better than the LSTM when classifying spam instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK tokenize vs keras tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought it could be interesting to compare the generalized NLTK tokenizer to the keras tokenizer.  We decided to compare them using basic LSTM networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "X_nltk = [word_tokenize(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = {}\n",
    "counter = 0\n",
    "def encode_sentence(seq):\n",
    "    global encoder, counter\n",
    "    fseq = []\n",
    "    for x in seq:\n",
    "        if x not in encoder:\n",
    "            encoder[x] = counter\n",
    "            counter+=1\n",
    "        fseq.append(encoder[x])\n",
    "    return fseq\n",
    "\n",
    "X_nltk = [encode_sentence(x) for x in X]\n",
    "X_nltk = pad_sequences(X_nltk, maxlen=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=len(X_nltk[0]),\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 910, 100)          900800    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 981,402\n",
      "Trainable params: 80,602\n",
      "Non-trainable params: 900,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rnn = Sequential()\n",
    "rnn.add(embedding_layer)\n",
    "rnn.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=metrics)\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X_nltk, y_ohe, test_size=0.2,\n",
    "                                                            stratify=y_ohe, \n",
    "                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4459 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4459/4459 [==============================] - 95s - loss: 0.3282 - precision: 0.8947 - acc: 0.8767 - val_loss: 0.1758 - val_precision: 0.9685 - val_acc: 0.9408\n",
      "Epoch 2/3\n",
      "4459/4459 [==============================] - 93s - loss: 0.2239 - precision: 0.9454 - acc: 0.9206 - val_loss: 0.2723 - val_precision: 0.9344 - val_acc: 0.9076\n",
      "Epoch 3/3\n",
      "4459/4459 [==============================] - 93s - loss: 0.1768 - precision: 0.9608 - acc: 0.9477 - val_loss: 0.2153 - val_precision: 0.9479 - val_acc: 0.9471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f901dcd12b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KerasGlove Published to PyPi\n",
    "I really liked being able to easily use glove embeddings with keras so I published a package to PyPi for it.  It's available under kerasglove and removes the need for a lot of the code in the notebook.  Here is a sample usage of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerasglove import GloveEmbedding\n",
    "EMBED_SIZE=100\n",
    "metrics = ['accuracy',precision]\n",
    "\n",
    "embed_layer = GloveEmbedding(\n",
    "                            EMBED_SIZE,\n",
    "                            MAX_TEXT_LEN,\n",
    "                            word_index)\n",
    "embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from kerasglove import GloveEmbedding\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(GloveEmbedding(EMBED_SIZE,\n",
    "                            MAX_TEXT_LEN,\n",
    "                            word_index))\n",
    "rnn.add(GRU(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=metrics)\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this is far easier to construct a network with a pre trained GloVe emebedding than doing it manually.\n",
    "\n",
    "The full source is here:\n",
    "https://github.com/LukeWoodSMU/KerasGlove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
