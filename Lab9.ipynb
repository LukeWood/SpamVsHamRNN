{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spam or Ham?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Lab Assignment Two: Exploring Text Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Justin Ledford, Luke Wood, Traian Pop \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Background\n",
    "SMS messages play a huge role in a person's life, and the confidentiality and integrity of said messages are of the highest priority to mobile carriers around the world. Due to this fact, many unlawful individuals and groups try and take advantange of the average consumer by flooding their inbox with spam, and while the majority of people successfully avoid it, there are people out there affected negatively by falling for false messages.  \n",
    "\n",
    "The data we selected is a compilation of 5574 SMS messages acquired from a variety of different sources, broken down in the following way: 452 of the messages came from the Grumbletext Web Site, 3375 of the messages were taken from the NUS SMS Corpus (database with legitimate message from the University of Singapore), 450 messages collected from Caroline Tag's PhD Thesis, and the last 1324 messages were from the SMS Spam Corpus v.0.1 Big. \n",
    "\n",
    "Overall there were 4827 \"ham\" messages and 747 \"spam\" messages, and about 92,000 words.\n",
    "\n",
    "### Purpose\n",
    "This data was collected initially for studies on deciphering the differences between a spam or ham (legitimate) messages. Uses for this research can involve advanced spam filtering technology or improved data sets for machine learning programs. However, a slight problem with this data set, as with most localized language-based data sets, is that due to the relatively small area of sampling, there are a lot of regional data points (such as slang, acronyms, etc) that can be considering \"useless\" data if a much more generalized data set is wanted. For our specific project however, we are keeping all this data in order for us to analyze it and get a better understanding of our data.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extracting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "descriptors_url = 'https://raw.githubusercontent.com/LukeWoodSMU/TextAnalysis/master/data/SMSSpamCollection'\n",
    "descriptors = requests.get(descriptors_url).text\n",
    "texts = []\n",
    "\n",
    "\n",
    "for line in descriptors.splitlines():\n",
    "    texts.append(line.rstrip().split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After the first look at the data we noticed a lot of phone numbers. Since almost every number was unique we concluded that the numbers were irrelevant to consider as words. We considered grouping all number tokens into one \"word\" and analyze the presence of words, but we decided to first start by just removing the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ham',\n",
       "  'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'),\n",
       " ('ham', 'Ok lar... Joking wif u oni...'),\n",
       " ('spam', 'Free entry in NUMBER_TOKEN'),\n",
       " ('ham', 'U dun say so early hor... U c already then say...'),\n",
       " ('ham', \"Nah I don't think he goes to usf, he lives around here though\"),\n",
       " ('spam', \"FreeMsg Hey there darling it's been NUMBER_TOKEN\"),\n",
       " ('ham',\n",
       "  'Even my brother is not like to speak with me. They treat me like aids patent.'),\n",
       " ('ham',\n",
       "  \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *NUMBER_TOKEN\"),\n",
       " ('spam',\n",
       "  'WINNER!! As a valued network customer you have been selected to receivea £NUMBER_TOKEN'),\n",
       " ('spam', 'Had your mobile NUMBER_TOKEN')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers\n",
    "texts = list(zip([a for a,b in texts], [re.sub('[0-9-]3+.*', ' ', b) for a,b in texts]))\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Converting the data from raw text into a sparse encoded bag-of-words representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create bag of words\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "bag_words = count_vect.fit_transform([t[1] for t in texts])\n",
    "count_vect_ham = CountVectorizer(stop_words='english')\n",
    "bag_words_ham = count_vect_ham.fit_transform([t[1] for t in texts if t[0] == 'ham'])\n",
    "count_vect_spam = CountVectorizer(stop_words='english')\n",
    "bag_words_spam = count_vect_spam.fit_transform([t[1] for t in texts if t[0] == 'spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Words counts per row in pandas dataframe\n",
    "df = pd.DataFrame(data=bag_words.toarray(),columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good    243\n",
       "like    244\n",
       "know    262\n",
       "free    268\n",
       "ll      269\n",
       "ok      292\n",
       "lt      316\n",
       "gt      318\n",
       "just    368\n",
       "ur      382\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out 10 most common words in our data\n",
    "df.sum().sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is interesting to see that 40% of the top words of our data set are \"slang\" words, as one would assume in a texting database there would be a higher percentage of abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Converting the data into a sparse encoded tf-idf representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>012number_token</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>...</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  000pes  008704050406  0089  0121  0125698789  012number_token  \\\n",
       "0  0.0  0.0     0.0           0.0   0.0   0.0         0.0              0.0   \n",
       "1  0.0  0.0     0.0           0.0   0.0   0.0         0.0              0.0   \n",
       "2  0.0  0.0     0.0           0.0   0.0   0.0         0.0              0.0   \n",
       "3  0.0  0.0     0.0           0.0   0.0   0.0         0.0              0.0   \n",
       "4  0.0  0.0     0.0           0.0   0.0   0.0         0.0              0.0   \n",
       "\n",
       "    02  0207 ...   zeros  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  \\\n",
       "0  0.0   0.0 ...     0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0   \n",
       "1  0.0   0.0 ...     0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0   \n",
       "2  0.0   0.0 ...     0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0   \n",
       "3  0.0   0.0 ...     0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0   \n",
       "4  0.0   0.0 ...     0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0   \n",
       "\n",
       "    ú1  〨ud  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "\n",
       "[5 rows x 8188 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tfidf\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english')\n",
    "tfidf_mat = tfidf_vect.fit_transform([t[1] for t in texts])\n",
    "tfidf_df = pd.DataFrame(data=tfidf_mat.toarray(), columns=tfidf_vect.get_feature_names())\n",
    "tfidf_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time     56.945668\n",
       "know     59.017660\n",
       "good     59.766063\n",
       "lt       64.356000\n",
       "ur       64.382773\n",
       "gt       64.679294\n",
       "come     67.380358\n",
       "just     72.266374\n",
       "ll       79.736271\n",
       "ok      103.308348\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out 10 most common words in our data\n",
    "tfidf_df.sum().sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When analyzing the top 10 most common words in our data with the TF-IDF representation, our top winners change. Instead of the previous \"ur\" being #1, it seems that it drops all the way down to #7, while \"ok\" jumps from #5 to #1. This means that \"ok\" is a more more relevant word throughout the documents compared to \"ur\" while \"ur\" itself just happens to have more individual hits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using tf-idf we can see which words are most relevant for ham and spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home      54.427173\n",
       "know      54.526118\n",
       "sorry     55.010803\n",
       "good      57.433290\n",
       "lt        62.619921\n",
       "gt        62.980016\n",
       "just      63.966590\n",
       "come      65.946440\n",
       "ll        77.769828\n",
       "ok       100.941869\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_ham = TfidfVectorizer(stop_words='english')\n",
    "tfidf_mat_ham = tfidf_vect_ham.fit_transform([t[1] for t in texts if t[0] == 'ham'])\n",
    "tfidf_df_ham = pd.DataFrame(data=tfidf_mat_ham.toarray(), columns=tfidf_vect_ham.get_feature_names())\n",
    "tfidf_df_ham.sum().sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reply     14.216356\n",
       "stop      14.292372\n",
       "claim     15.725948\n",
       "urgent    15.902412\n",
       "prize     16.668448\n",
       "text      17.226986\n",
       "txt       18.288373\n",
       "ur        18.304256\n",
       "mobile    18.412027\n",
       "free      26.184182\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_spam = TfidfVectorizer(stop_words='english')\n",
    "tfidf_mat_spam = tfidf_vect_spam.fit_transform([t[1] for t in texts if t[0] == 'spam'])\n",
    "tfidf_df_spam = pd.DataFrame(data=tfidf_mat_spam.toarray(), columns=tfidf_vect_spam.get_feature_names())\n",
    "tfidf_df_spam.sum().sort_values()[-10:]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
